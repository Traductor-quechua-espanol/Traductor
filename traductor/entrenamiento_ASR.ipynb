{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Creacion de dataset con Hugging Face",
   "id": "4ae68b6059382621"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-02T00:01:45.865657Z",
     "start_time": "2025-10-02T00:00:50.517700Z"
    }
   },
   "source": [
    "from datasets import Dataset\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import re"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\asr-quechua\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\envs\\asr-quechua\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T00:01:45.963558Z",
     "start_time": "2025-10-02T00:01:45.883252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"audio_transcripcion.csv\", sep=\";\", encoding=\"utf-8\")\n",
    "print(df.head())"
   ],
   "id": "57c1dd2558aa9086",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            path  \\\n",
      "0  common_voice_quy_41903591.wav   \n",
      "1  common_voice_quy_41903592.wav   \n",
      "2  common_voice_quy_41903593.wav   \n",
      "3  common_voice_quy_41903594.wav   \n",
      "4  common_voice_quy_41903595.wav   \n",
      "\n",
      "                                            sentence  \n",
      "0              Akchiyqa kay puykunamantam paqarimun.  \n",
      "1  Utqayllam wiñanku, sisarinku, hinaspa pawayta ...  \n",
      "2         Maynintam chayayman, puquio llaqtallayman.  \n",
      "3  Manaña sachakuna kaptin, pachaqa chinkariyta q...  \n",
      "4             Kaykunam hukniraq kallpasapa yurakuna.  \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T00:01:46.118245Z",
     "start_time": "2025-10-02T00:01:46.084551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "AUDIO_DIR = \"segmentacion_vad\"\n",
    "\n",
    "df[\"path\"] = df[\"path\"].apply(lambda x: os.path.join(AUDIO_DIR, x))"
   ],
   "id": "841ade7633757694",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T00:01:46.212578Z",
     "start_time": "2025-10-02T00:01:46.154755Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = Dataset.from_pandas(df)",
   "id": "c712347e843223a2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T00:01:46.291598Z",
     "start_time": "2025-10-02T00:01:46.224975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_ds, test_ds = dataset[\"train\"], dataset[\"test\"]"
   ],
   "id": "271c02fe185f97c3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T00:01:46.324844Z",
     "start_time": "2025-10-02T00:01:46.312776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_text(example):\n",
    "    text = example[\"sentence\"].lower()\n",
    "    text = re.sub(r\"[^a-zñáéíóúü\\s]\", \"\", text)\n",
    "    example[\"sentence\"] = text.strip()\n",
    "    return example"
   ],
   "id": "8e7e4ac235780ac4",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T00:01:46.928188Z",
     "start_time": "2025-10-02T00:01:46.341447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_ds = train_ds.map(clean_text)\n",
    "test_ds = test_ds.map(clean_text)"
   ],
   "id": "71ae533133fb988f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1153/1153 [00:00<00:00, 2366.24 examples/s]\n",
      "Map: 100%|██████████| 129/129 [00:00<00:00, 2577.91 examples/s]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tokenizador + Procesador",
   "id": "691ed3b1b1f1ea50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T00:01:47.014504Z",
     "start_time": "2025-10-02T00:01:46.948333Z"
    }
   },
   "cell_type": "code",
   "source": "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2Processor",
   "id": "9fa311f06df82311",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T00:01:47.125066Z",
     "start_time": "2025-10-02T00:01:47.028699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_text = \" \".join(train_ds[\"sentence\"])\n",
    "vocab = list(set(all_text))\n",
    "vocab_dict = {ch: i for i, ch in enumerate(sorted(vocab))}\n",
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]"
   ],
   "id": "3337ce2802754fb0",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T00:01:47.189310Z",
     "start_time": "2025-10-02T00:01:47.159276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "with open(\"vocab.json\", \"w\", encoding=\"utf-8\") as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file, ensure_ascii=False)"
   ],
   "id": "fce70c03f7209d7a",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T00:01:47.248243Z",
     "start_time": "2025-10-02T00:01:47.213486Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = Wav2Vec2CTCTokenizer(\"vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")",
   "id": "6c471e0675fffee",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T00:01:47.281573Z",
     "start_time": "2025-10-02T00:01:47.272503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True)"
   ],
   "id": "ad27598c1276be52",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T00:01:47.315458Z",
     "start_time": "2025-10-02T00:01:47.296811Z"
    }
   },
   "cell_type": "code",
   "source": "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)",
   "id": "b78f6e88909ae7d6",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Preparación de datos",
   "id": "fb05f44eae6e967c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T00:01:47.361377Z",
     "start_time": "2025-10-02T00:01:47.344741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import librosa\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    speech_array, sampling_rate = librosa.load(batch[\"path\"], sr=16000)\n",
    "\n",
    "    batch[\"input_values\"] = processor(speech_array, sampling_rate=16000).input_values[0]\n",
    "\n",
    "    with processor.as_target_processor():\n",
    "        batch[\"labels\"] = processor(batch[\"sentence\"]).input_ids\n",
    "\n",
    "    return batch"
   ],
   "id": "35ba6132f32f5cf9",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T00:03:50.674360Z",
     "start_time": "2025-10-02T00:01:47.388590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_ds = train_ds.map(prepare_dataset, remove_columns=train_ds.column_names)\n",
    "test_ds = test_ds.map(prepare_dataset, remove_columns=test_ds.column_names)"
   ],
   "id": "9c90de8720284cfb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1153 [00:00<?, ? examples/s]C:\\Users\\user\\anaconda3\\envs\\asr-quechua\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:180: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 1153/1153 [01:58<00:00,  9.71 examples/s]\n",
      "Map: 100%|██████████| 129/129 [00:04<00:00, 29.12 examples/s]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Entrenamiento",
   "id": "30f01077b0735e05"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T00:03:51.204157Z",
     "start_time": "2025-10-02T00:03:50.940521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, TrainingArguments, Trainer"
   ],
   "id": "5c7c774af93a04e7",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T00:04:03.402428Z",
     "start_time": "2025-10-02T00:03:51.236834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    \"facebook/wav2vec2-base\",\n",
    "    vocab_size=len(tokenizer)\n",
    ")"
   ],
   "id": "15e764d55f2f0d87",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\asr-quechua\\lib\\site-packages\\transformers\\configuration_utils.py:335: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T00:04:03.437129Z",
     "start_time": "2025-10-02T00:04:03.422050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import accelerate\n",
    "print(accelerate.__version__)\n",
    "print(accelerate.__file__)"
   ],
   "id": "5872384b9edfa9c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1\n",
      "C:\\Users\\user\\anaconda3\\envs\\asr-quechua\\lib\\site-packages\\accelerate\\__init__.py\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T00:38:18.031362Z",
     "start_time": "2025-10-02T00:38:17.654386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./asr_quechua\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    save_strategy=\"steps\",\n",
    "    num_train_epochs=5,\n",
    "    fp16=False,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=50,\n",
    "    eval_steps=100,\n",
    "    save_steps=100,\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.005,\n",
    ")"
   ],
   "id": "97a306e85ae7eaaa",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T00:38:25.498651Z",
     "start_time": "2025-10-02T00:38:22.526400Z"
    }
   },
   "cell_type": "code",
   "source": "wer_metric = evaluate.load(\"wer\")",
   "id": "a66fae26f1101516",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T00:38:27.922095Z",
     "start_time": "2025-10-02T00:38:27.912728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = torch.argmax(torch.tensor(pred.predictions), dim=-1)\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "    return {\"wer\": wer_metric.compute(predictions=pred_str, references=label_str)}"
   ],
   "id": "a711e9c7616bc47",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T00:38:30.051144Z",
     "start_time": "2025-10-02T00:38:30.038667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "class DataCollatorCTCWithPadding:\n",
    "    def __init__(self, processor, padding=True):\n",
    "        self.processor = processor\n",
    "        self.padding = padding\n",
    "\n",
    "    def __call__(self, features):\n",
    "\n",
    "        input_features = [{\"input_values\": f[\"input_values\"]} for f in features]\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "\n",
    "        batch_inputs = self.processor.feature_extractor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        with self.processor.as_target_processor():\n",
    "            batch_labels = self.processor.tokenizer.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "        labels = batch_labels[\"input_ids\"].masked_fill(\n",
    "            batch_labels.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        batch = {\n",
    "            \"input_values\": batch_inputs[\"input_values\"],\n",
    "            \"labels\": labels\n",
    "        }\n",
    "        return batch\n"
   ],
   "id": "4b09d9b7cebb8c87",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-02T00:38:35.028532Z",
     "start_time": "2025-10-02T00:38:34.971344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ],
   "id": "5515a71db7fbdf59",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_17444\\2563651377.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-10-02T00:38:38.223697Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "8846391fcd0b9f6a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
